---
title: "Intro to tidymodels and XGBoost in R"
author: "J. Harrison"
date: "7/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction
So, sklearn is a pretty cool Python package for doing machine learning. However, the tidymodels library for R seems like it may be a nice alternative if one wants to stay in R. It seems to offer similarly convenient tools for feature engineering and various data preprocessing, but relies on the tidyverse ease-of-use-aesthetic.

As an aside, tidymodels has some really good documentation. Kudos to them for making software docs that are actually enjoyable to read and easy to understand.

[HERE](https://cimentadaj.github.io/blog/2020-02-06-the-simplest-tidy-machine-learning-workflow/the-simplest-tidy-machine-learning-workflow/) is a nice, critical review of tidymodels, with reference to its predecessor, the caret package. The author presents a minimal example and discusses how tidymodels many packages are a bit cumbersome...it is useful because most of what one finds about tidymodels is very positive and so it is hard for a new user to see potential drawbacks or ways the software can (probably will) be improved. 

# Installing all the things...

tidymodels imports a lot of stuff (see the chunk), but much of it is standard for the tidyverse (e.g., ggplot, tidyr, etc.). Packages that are specific to the tidymodels framework include the following:

* dials - a package for tuning model parameters
* parsnip - provides a common interface for models, e.g., different decision-tree model software may say ntree, treen, num_tree all to specify the number of trees. Remembering the differences among tools, or looking them up, is not a great use of time...enter parsnip.
* tune - also useful for tuning models, has stuff to help with pre- and post-processing data and results
* workflows - allows models and preprocessing steps to be contained in a workflow object. Analogous to pipelines in sklearn.
* workflowsets - makes a bunch of workflows
* yardstick - model evaluation tools

FYI if you are on an Apple the gower dependency probably won't install from source. The OS X compiler doesn't support openMP and so compilation will probably fail. Just install gower without compilation to get round this issue.
```{r}
#install.packages("gower")
#install.packages("tidymodels")
#Tidymodels imports all this: 
# broom (≥ 0.7.6), cli (≥ 2.4.0), conflicted (≥ 1.0.4), dials (≥ 0.0.9), dplyr (≥ 1.0.5), ggplot2 (≥ 3.3.3), infer (≥ 0.5.4), modeldata (≥ 0.1.0), parsnip (≥ 0.1.5), purrr (≥ 0.3.4), recipes (≥ 0.1.16), rlang (≥ 0.4.10), rsample (≥ 0.0.9), rstudioapi (≥ 0.13), tibble (≥ 3.1.0), tidyr (≥ 1.1.3), tune (≥ 0.1.3), workflows (≥ 0.2.2), workflowsets (≥ 0.0.2), yardstick (≥ 0.0.8)
 	
library(tidymodels)

#install.packages("xgboost")
library(xgboost)

# unleash the beast (actually, I need to learn more about this package and when it is helpful...hah)
#install.packages(doParallel)
# library(doParallel)
# all_cores <- parallel::detectCores(logical = FALSE)
# registerDoParallel(cores = all_cores)

set.seed(666)
```
Load some data and do a bit of minor wrangling. Note that I make no effort at code efficiency here, nor do I bother with exploratory data analysis for this example.
```{r}
X <- read.csv("./imputed_scaled_ITS_metadata.csv")
X <- X[,names(X) != "shannonsISD"] #remove microbial diversity

#Bring in taxon proportions, these were wrangled in R after being generated by CNVRG
taxa <- read.csv("./ITSp_estimates_wrangled_for_post_modeling_analysis_divided_by_ISD.csv")
#Get an abundant taxon.
focaltaxon <- "Zotu62"

X$compartment <- ifelse(X$EN == 1, "EN", NA)
X$compartment[X$EP == 1] <- "EP"
table(X$compartment)

#Make sure our data are in the same order
X$sample <- paste(X$plant.x, X$compartment, sep = "_")
table(X$sample %in% taxa$sample)
#the missing stuff were things I removed due to poor sequencing
merged_dat <- merge(X, taxa, by.x = "sample", by.y = "sample")

#Making a response object for readability
response_taxon <- merged_dat[, names(merged_dat) == focaltaxon]

#Get rid of stuff we don't need in merged_dat (all the bogus stuff was from merging with taxa, so we can index by og dimensions)
merged_dat <- data.frame(response_taxon,
                         merged_dat[,1:length(names(X))])
table(merged_dat$sample %in% taxa$sample)

```
Let's do a simple train test split of the data, while stratifying by compartment and presence of the focal taxon. I stratify via two things by pasting them together.
```{r}
focal_one_hot <- ifelse(response_taxon > 0.003, 1, 0)
table(focal_one_hot)
merged_dat$stratify <- paste(merged_dat$compartment, focal_one_hot)
merged_dat_split <- rsample::initial_split(
  merged_dat, 
  prop = 0.2, 
  strata = stratify
)
```
Note that I already did a lot of feature engineering on these (imputing, etc.), so my demo of how to do preprocessing will not include some common tasks. For examples of how to do many common thing, see [HERE](https://www.r-bloggers.com/2020/05/using-xgboost-with-tidymodels/) and [HERE](https://www.tidymodels.org/start/recipes/).

From the help, "A 'recipe' is a description of the steps to be applied to a data set in order to prepare it for data analysis." To initiate a recipe we use R's standard formula syntax:
```{r}
phyllo_recipe <- 
  recipe(response_taxon ~ ., data = training(merged_dat_split)) 
```
This object is a tibble and looks like this
```{r}
summary(phyllo_recipe)
```
Note the column for 'role' this holds info about what the feature is for. Predictor and outcome are the critical ones, but we can add any roles we want. Roles aside from the predictor and outcome roles are just strings defined by the user for the user. There isn't a long list of roles to memorize, the only ones that do anything are predictor and outcome. 

The help docs demonstrate a useful example where they specify an "ID" role. Thus a variable that contains sample info can be retained in the dataset, but not included as a predictor. This is handy because sub-setting data to remove identifiers and then putting them back in can lead to critically damning errors if one gets anything out of order. I like that one can sidestep that whole issue by just designating roles up front. Note, from my reading I do not think sklearn has anything analogous to this (as of 2021) and I think this handy organization could be a nice time saver and a way to avoid fragility imposed via lots of indexing and splitting/merging of data.
```{r}
#adding some roles
phyllo_recipe <- 
  recipe(response_taxon ~ ., data = training(merged_dat_split)) %>%
          update_role(
            #Can do via indexing, but am writing names for clarity
            #names(merged_dat)[1:11], #stuff we want to recode the role of
            sample, 
            X.1.x,
            region_site,
            X.x,
            taxon.x,
            region_site_plant,
            plant.x,
            forward_barcode,
            reverse_barcode,
            locus,
            samplename,
            stratify, 
           # X.1.y, 
            new_role = "ID")
summary(phyllo_recipe)
```
Now all of our bookkeeping crap is coded as "ID" and won't get processed during feature engineering, necessarily, pretty handy!

Note that I already turned my categoricals into one-hot encoded variables, but tidymodels has some convenient ways to do this too. They do simple, helpful things like remove the original categorical variable from the dataset. Lots of convenience, particularly for putting one-hot encoded variables back together and defining interactions among them as yet more dummy coded variables. See [HERE](https://recipes.tidymodels.org/articles/Dummies.html).

Next, we need to prep to run our recipe. From prep's help: "For a recipe with at least one preprocessing operation, estimate the required parameters from a training set that can be later applied to other data sets."

If you have a recipe that requires any sort of preprocessing you will have to run prep before you move on.
```{r}
phyllo_recipe <- prep(phyllo_recipe)
```
Alright, once you have a recipe sorted and your ingredients prepped you got to put it in the oven! Seriously, the command used to execute a recipe is called bake.
```{r}
phyllo_chopped <- bake(
    phyllo_recipe, #the recipe
    new_data = training(merged_dat_split) #these are the data that will be processed
  ) %>%  
  rsample::vfold_cv(v = 2) #only using two so this code doesn't take as long to run.
```
Alright, this is starting to smell tasty (to continue the analogy to an unfortunate degree). Next, we define the model. Note that we use parsnip to do this. Recall that parsnip is a part of the tidymodels ecosystem that provides a standard API for various models. All the calls to tune() are the model parameters that we will want to tune. During the set_engine command we are saying that we want to use XGBoost to run our model and we want squared error to be our measure of success. For different engines you can pass in specific options as needed. 

The next few chunks were lifted from [HERE](https://www.r-bloggers.com/2020/05/using-xgboost-with-tidymodels/).
```{r}
xgboost_model <- 
  parsnip::boost_tree(
    mode = "regression",
    trees = 1000,
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    loss_reduction = tune()
  ) %>%
    set_engine("xgboost", objective = "reg:squarederror")
```
Next, set up the parameter space to explore. This is where dials comes in. 
```{r}
xgboost_params <- 
  dials::parameters(
    min_n(),
    tree_depth(),
    learn_rate(),
    loss_reduction()
  )
```
Then we specify how to explore this space. From the help, "The types of designs supported here are Latin hypercube designs and designs that attempt to maximize the determinant of the spatial correlation matrix between coordinates."

We are using the latter, max entropy approach. This is a way to try and make sure that the choices we make are far apart and explore a lot of the parameter space. I am not sure what a Latin hypercube is yet.
```{r}
xgboost_grid <- 
  dials::grid_max_entropy(
    xgboost_params, 
    size = 60
  )
```
To see what we are gonna try, we can use this command.
```{r}
knitr::kable(head(xgboost_grid))
```
We define a workflow next. A workflow is another convenience feature. From the ever-helpful help: 

"A workflow is an object that can bundle together your pre-processing, modeling, and post-processing requests. For example, if you have a recipe and parsnip model, these can be combined into a workflow. The advantages are:

* You don’t have to keep track of separate objects in your workspace.
* The recipe prepping and model fitting can be executed using a single call to fit().
* If you have custom tuning parameter settings, these can be defined using a simpler interface when combined with tune.
* In the future, workflows will be able to add post-processing operations, such as modifying the probability cutoff for two-class models."

Seems fairly rad. Here is how to make one.
```{r}
  xgboost_wf <- workflows::workflow() %>%
      add_model(xgboost_model) %>% 
      add_formula(response_taxon ~ .)
```
Now we do some tuning. THIS IS A BIG WIN OVER SKLEARN. The Hyperopt package for sklearn allows for smart hyper parameter tuning, and isn't too hard to figure out, but this is way easier. Also, getting the user-friendly hyperopt wrapper hpsklearn installed can be a bit tricky and caused problems for me on Teton. This takes a lot less effort all around. 
```{r}
xgboost_tuned <- tune::tune_grid(
  object = xgboost_wf,
  resamples = phyllo_chopped,
  grid = xgboost_grid,
  metrics = yardstick::metric_set(rmse, rsq, mae),
  control = tune::control_grid(verbose = TRUE)
)
```
FAILING HERE, probably bc I have one level or an NA in one of my factors after splitting. I am leaving this issue in the Rmd, since others may experience it as well. The notes state, "This tuning result has notes. Example notes on model fitting include: preprocessor 1/1: Error in stats::`contrasts<-`(x, how_many, value): contrasts can be applied only to factors with 2 or more levels"

I suspect this is because one of the recoded categoricals has only one level after splitting the data.

```{r}
#resplit for debugging
phyllo_chopped <- bake(
    phyllo_recipe, 
    new_data = training(merged_dat_split)
  ) %>%  
  rsample::vfold_cv(v = 2)

#Look for single level factors and remove
remover <- function(toexamine) {
  toremove <- NA
  for (i in 1:(length(toexamine))) {
    if (1 %in% names(table(toexamine[, i]))) {
      next
    }
    else{
      # print("hold up")
      # print(names(toexamine)[i])
      # print(table(toexamine[, i]))
      toremove <- append(toremove, names(toexamine)[i])
    }
  }
  toexamine <- toexamine[,!(names(toexamine) %in% toremove)]
  return(toexamine)
}

#clean up our splits.
for(i in 1:length(phyllo_chopped$splits)){
  #This mess...note the -1 so that I don't chop off the response variable
  out <- remover(phyllo_chopped$splits[[i]]$data[,min(grep("Abies", names(merged_dat))):length(merged_dat)-1 ])
  phyllo_chopped$splits[[i]]$data <- phyllo_chopped$splits[[i]]$data[,-(min(grep("Abies", names(merged_dat))):length(merged_dat)-1)]
  phyllo_chopped$splits[[i]]$data <- data.frame(phyllo_chopped$splits[[i]]$data, out)
}
```
This alone doesn't fix things. Same error if we run the tuning code. Upon further looking, it turns out that the roles for bookkeeping factors are not being inherited, so the split data have columns in them that have problematic factors levels. Let's chop them off and see what happens.

```{r}
for(i in 1:length(phyllo_chopped$splits)){
  phyllo_chopped$splits[[i]]$data <- data.frame(phyllo_chopped$splits[[i]]$data[, 12:length(phyllo_chopped$splits[[i]]$data)])
}
```
That fixes things. This is disappointing, would be cool if the tools of rsample were in tidymodels so that roles could be inherited.

Anyway, its not too hard to remember to chop useless shit out of the dataframe prior to tuning. And tuning is still way easier than with sklearn...
```{r,results='hide', message=FALSE, warning=FALSE}
xgboost_tuned <- tune::tune_grid(
  object = xgboost_wf,
  resamples = phyllo_chopped,
  grid = xgboost_grid,
  metrics = yardstick::metric_set(rmse, rsq, mae),
  control = tune::control_grid(verbose = TRUE)
)
```
See what worked best...from help, "show_best() displays the top sub-models and their performance estimates."
```{r}
xgboost_tuned %>%
  tune::show_best(metric = "rmse") %>%
  knitr::kable()
```
"select_best() finds the tuning parameter combination with the best performance values."
```{r}
xgboost_best_params <- xgboost_tuned %>%
  tune::select_best("rmse")
knitr::kable(xgboost_best_params)
```
Feed these best parameters into our model...
```{r}
#finalize model parameters
xgboost_model_final <- xgboost_model %>% 
  finalize_model(xgboost_best_params)
#finalize workflow
xgboost_wf <- workflows::workflow() %>%
      add_model(xgboost_model_final) %>% 
      add_formula(response_taxon ~ .)
```
Alright, time to train and fit the model. We will train and fit to the training data, then fit to the test data. 
```{r}
#Rebake, bc last time we piped through to rsample. The roles are still not preserved...
train_processed <- bake(
    phyllo_recipe, #the recipe
    new_data = training(merged_dat_split) #these are the data that will be processed
  )

#Note the indexing nonsense bc the roles are not inherited
train_prediction <- xgboost_model_final %>%
  # fit the model on all the training data
  fit(
    formula = response_taxon ~ ., 
    data    = train_processed[,12:length(train_processed)]
  ) %>%
  predict(new_data = train_processed[,12:length(train_processed)]) %>%
  bind_cols(train_processed[,12:length(train_processed)])
```

```{r}
xgboost_score_train <- 
  train_prediction %>%
  yardstick::metrics(response_taxon, .pred) %>%
  mutate(.estimate = format(round(.estimate, 2), big.mark = ","))
knitr::kable(xgboost_score_train)
```
Look at that champion r squared! Hah. Lets try on the test data. 
```{r}
test_processed <- bake(
    phyllo_recipe, 
    new_data = testing(merged_dat_split) 
  )

test_prediction <- xgboost_model_final %>%
  fit(
    formula = response_taxon ~ ., 
    data    = test_processed[,12:length(test_processed)]
  ) %>%
  predict(new_data = test_processed[,12:length(test_processed)]) %>%
  bind_cols(test_processed[,12:length(test_processed)])

xgboost_score_test <- 
  test_prediction %>%
  yardstick::metrics(response_taxon, .pred) %>%
  mutate(.estimate = format(round(.estimate, 2), big.mark = ","))
knitr::kable(xgboost_score_test)
```
Woot! Science paper forthcoming!!  

Let's look at variable importance. 

```{r}
#install.packages("vip")
library(vip)

#This code requires a workflow be passed in.
xgboost_wf %>%
  fit(
      data = test_processed[,12:length(test_processed)]) %>%
  extract_fit_parsnip() %>%
  vip(geom = "point")
```

Try and extract model object and predicted values while avoiding the cumbersome piping nonsense. 
```{r}
#cor.test(test_prediction$.pred, test_prediction$response_taxon)
par(mfrow = c(1,2))
plot(test_prediction$.pred, test_prediction$response_taxon)
hist( test_prediction$response_taxon, main = "")
```

Maybe the prediction is so good because the taxon is hardly ever there! Yup.
```{r}
summary(test_prediction$response_taxon)
table(test_prediction$response_taxon > 1) #greater than the ISD
```
So, this is a wacky case... so a great way to see how tidymodels works when things aren't in toy data land. I will need to figure out better ways to stratify the data I think.

# Summary

tidymodels has some real nice advantages and I think it is pretty damn sweet. 

Pros include:

* Great documentation and a large number of easy reading blog posts to help.
* Since it is in R we get some of the nice things about R including a good IDE, simplified environment issues compared to Python, and better plotting options (at least given my background).
* I like the roles feature during recipe building, but, sadly, this is not inherited. 
* Very little cognitive load to do a lot of complicated stuff. Writing code to do feature engineering, cross validation, tuning, etc. is a lot and we don't need to do it anymore. 
* Simpler to use and install than sklearn, particularly regarding tuning as the Hyperopt package and the hpsklearn packages are a bit tricky.

Cons include: 

* Because some of the functions do so much it was a bit tricky for me to know for sure that everything that I wanted to happen did indeed happen. A verbose option would help that prints to stdout all the things happening in easy to use terms. As is, I found myself poking around in objects to see what was what (hidden from this markdown)
* Some functions appear to expect piping and don't produce outputs that are easy to parse outside of the tidymodels functions. I don't like to have to pipe as it can make debugging harder and makes code harder to read. A little piping is fine, but one shouldn't have to pipe and I found at least one case where the object output by a function was sort of opaque unless it was piped into some other function. 
* There are a lot of packages within the tidymodels universe and it seems like they should be in one bigger package to me, thus streamlining dependency issues and making updates easier. I am sure they did what they did for a reason though..
* I was hoping stuff from the recipe would be inherited to other functions, like the roles. If all was in one package this would be easier because then one could expect inheritance among functions within the package but not outside of the package, and it would be easier to tell which was which.

Things yet to learn:

* Unsure about scalability in terms of support for parallel processing, depending on the model. 
* I don't yet know how to pass in specific ranges for tuning options. I suspect this is simple...





